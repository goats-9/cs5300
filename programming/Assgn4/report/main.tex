\documentclass[conference,compsoc]{IEEEtran}
\usepackage{./macros}

\begin{document}
\title{CS5300 Assignment 4\\Comparing Obstruction-Free and Wait-Free Snapshot Algorithms}
\author{Gautam Singh\\CS21BTECH11018}
\maketitle
\tableofcontents

\bigskip

\begin{abstract}
    This report analyzes C++ implementations of Multi-Reader Multi-Writer (MRMW)
    obstruction-free and wait-free snapshot objects. In particular, we compare
    the two implementations with regards to scalability and impact of the update
    operation on snapshot collection in the average and worst cases. The rest of
    the report is organized as follows. In \autoref{sec:prog-design}, we
    describe the low-level program design on which experiments are run. In
    \autoref{sec:results}, we compare the performance of both implementations
    with varying parameters. Finally, we conclude the report in
    \autoref{sec:conclusion}.
\end{abstract}

\section{Program Design}
\label{sec:prog-design}

We give here an overview of the implementation details of the two programs, one
for the obstruction-free snapshot object and one for the wait-free snapshot
object.

\subsection{Stamped Values}
Though both implementations use different stamped value types, we use the one
described in the wait-free snapshot implementation, which helps in the MRMW
implementation of both snapshot objects. The implementation is shown in
\autoref{code:stamped-value}.

\begin{listing}[!ht]
    \inputminted{cpp}{codes/StampedValue.cpp}
    \caption{Implementation of Timestamped Value.}
    \label{code:stamped-value}
\end{listing}

The stamped value contains the following fields.

\begin{enumerate}
    \item \texttt{value}: This is where the actual value is stored.
    \item \texttt{id}: This is the ID of the thread that last wrote to this
    variable.
    \item \texttt{stamp}: This is the timestamp that is used to determine when
    the value was written. Notice that since we are also storing the thread ID,
    the stamps may simply be a thread-local sequence number, since the pair
    (\texttt{id}, \texttt{stamp}) will be unique. This also avoids the use of
    shared atomic counters among threads, at the cost of smaller storage for the
    value.
\end{enumerate}

To ensure that this data type can be made atomic, we must ensure its total size
is at most 64 bytes, since most architectures have 64-bit registers. In our
implementation, \texttt{T} can only be at most 32 bytes. For this application,
we set \texttt{T} to be an unsigned 32-bit integer (\texttt{uint32\_t} in C++).

\subsection{Implementation of Snapshot Algorithms}
\label{subsec:snap-interface}

Both snapshot implementations follow the interface given in
\autoref{code:mrmw-snapshot-interface}.

\begin{listing}[!ht]
\inputminted{cpp}{codes/Snapshot.cpp}
\caption{C++ MRMW snapshot object interface.}
\label{code:mrmw-snapshot-interface}
\end{listing}

In C++, atomic data types are not copyable or movable. Thus, we cannot create or
create or store a C++ array/vector of atomic objects. Instead, we create a
shared array of pointers to these atomic objects, dereferencing them when we
need to access the atomic stamped value they are pointing to.

\subsection{Threads and Runner Functions}
\label{subsec:threads-and-runners}
Threads are created using the C++ \texttt{std::thread} class. This makes it
convenient to pass thread IDs and references to other objects using
\texttt{std::ref} to the thread runner functions. In particular, we pass the
following arguments to the runner function.

\begin{enumerate}
    \item Thread ID from 0 to \(N - 1\), where \(N\) is the number of threads.
    This is because the \texttt{std::thread} class creates threads with IDs that
    may not be in this range, as they are only meant to be unique. However,
    these IDs are only used in logging and not in the snapshot algorithms.
    \item Reference to the MRMW snapshot object.
    \item Logging output stream (a reference to a C++ \texttt{std::stringstream}
    object) to write timing information for further analysis and output.
\end{enumerate}

Threads created using the \texttt{std::thread} class have a thread ID that may
not fit into a 16-bit unsigned integer. Therefore, we use \texttt{std::hash} to
hash it to this range. This hashed ID is then used in the snapshot algorithms. A
limitation of doing this is that the algorithm can work only with upto
\(2^{16}\) threads correctly, since after that there will exist two threads that
are mapped to the same hashed 16-bit ID.

\subsection{Timing}
\label{subsec:timing}

The timestamps are reported using the \texttt{std::chrono} library. To prevent
zero values, we report times in nanoseconds, which is the smallest available
unit of time. However, results in the analysis are suitably scaled and reported
in milliseconds where needed.

\subsection{Random Number Generation}
\label{subsec:rng}

Random delays for simulating sleep times of update and scan threads are
generated using exponential distributions with mean \(\mu_w\) and \(\mu_s\)
respectively. This is done using the \texttt{std::exponential\_distribution}
class. Further, the locations and 32-bit values for update threads are generated
using uniform integer distributions, provided by the
\texttt{std::uniform\_int\_distribution} class. The randomness is generated
using a Mersenne Twister, instantiated using an \texttt{std::mt19937} object.
This random number generator is seeded using the current time since epoch.

\section{Results and Analysis}
\label{sec:results}

In this section, we analyze the performance of the snapshot algorithms on
metrics such as scalability and impact of constant updates on snapshot in the
average case and the worst case. This application was run on the CSE servers in
IITH.

\subsection{Scalability}
\label{subsec:scalability}

As the number of threads increases, we expect the time taken for the snapshot in
either algorithm to increase, since more update threads can move, causing the
snapshot algorithm to run for longer. The average case and worst case
scalability of the algorithms are given in \autoref{fig:exp1} and
\autoref{fig:exp2}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{images/exp1.png} 
    \caption{Average case scalability with varying number of threads.}
    \label{fig:exp1}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{images/exp2.png}
    \caption{Worst case scalability with varying number of threads.}
    \label{fig:exp2}
\end{figure}

We observe the following.
\begin{enumerate}
    \item In the average and worst case, the obstruction-free snapshot is faster
    and more scalable than the wait-free snapshot.
    \item In the worst case, both snapshot algorithms exhibit a worst-case
    running time approximately linear with the total number of threads.
    \item The gap between the times taken for the update and snapshot operation
    is huge for the obstruction-free snapshot and small for the wait-free
    snapshot in the average case.
    \item The reason for poor performance and scalability of the wait-free
    snapshot is because of the fact that update threads have to perform a
    snapshot as well to ensure wait-freedom, which is not the case for the
    obstruction-free snapshot.
\end{enumerate}

\subsection{Impact of Update on Scan}
\label{susbec:impact-update-scan}

As the number of update threads per scan thread increases, there are more
threads that can obstruct the snapshot operation, thus we should expect the
snapshot operation to take longer for a larger \(\frac{n_w}{n_s}\), the number
of update to scan threads. The average and worst case analysis are shown in
\autoref{fig:exp3} and \autoref{fig:exp4}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{images/exp3.png} 
    \caption{Average case impact of update on snapshot with varying \(\frac{n_w}{n_s}\).}
    \label{fig:exp3}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{images/exp4.png} 
    \caption{Worst case impact of update on snapshot with varying \(\frac{n_w}{n_s}\).}
    \label{fig:exp4}
\end{figure}

We observe the following.
\begin{enumerate}
    \item The wait-free scan is only marginally faster than the obstruction-free
    scan in both the average case and worst case. A reason for that could be the
    exponential sleep time that prevents the scan from being obstructed.
    \item In this setting, due to the larger number of update threads, the
    update threads are the bottleneck for both algorithms.
    \item Here, the wait-free snapshot performs better with larger number of
    update threads since there is more chance for update threads to move. This
    makes the obstruction-free algorithm slower.
\end{enumerate}

\section{Conclusion}
\label{sec:conclusion}

From the analysis, we conclude that even though there are no wait-free
guarantees, the obstruction-free snapshot algorithm seems to scale better than
the wait-free snapshot algorithm. However, if there is a larger proportion of
update threads, then the wait-free snapshot performs better. An analysis with a
larger shared array may be required to fully assess the speed of the
obstruction-free snapshot algorithm in practice since the runtimes of both
algorithms are comparable.

\end{document}